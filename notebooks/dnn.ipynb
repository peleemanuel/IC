{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Încarcă setul de date CSV într-un DataFrame\n",
    "df = pd.read_csv(\"../data/final_dataset_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numărul de componente după PCA: 13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Separă caracteristicile de variabila țintă (dacă există)\n",
    "# Excludem coloana 'price' care este variabila țintă\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']  # Definim variabila țintă\n",
    "\n",
    "# Standardizăm datele\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Aplicăm PCA\n",
    "pca = PCA(n_components=13)  # Specificăm să reducem la 13 componente\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Verificăm câte componente ne-au rămas\n",
    "print(\"Numărul de componente după PCA:\", X_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Convertim numpy arrays în tensori PyTorch\n",
    "X_tensor = torch.tensor(X_pca, dtype=torch.float32)\n",
    "# Asigură-te că 'y' este un array\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "# Creăm TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setăm dimensiunile pentru antrenament și validare\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Împărțim dataset-ul\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specificăm dimensiunea lotului\n",
    "batch_size = 64\n",
    "\n",
    "# Creăm DataLoader pentru antrenament și validare\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 13]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    # Ar trebui să arate forma (batch_size, 13) și (batch_size,)\n",
    "    print(inputs.shape, targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definirea modelului DNN\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        # Inițializarea layerelor\n",
    "        # Primul layer primeste 13 caracteristici (după PCA)\n",
    "        self.fc1 = nn.Linear(13, 20)\n",
    "        # self.fc2 = nn.Linear(30, 30)  # Al doilea layer\n",
    "        # self.fc3 = nn.Linear(128, 1)  # Al treilea layer\n",
    "        # self.fc4 = nn.Linear(30, 30)  # Al patrulea layer\n",
    "        # Layerul de output cu o singură valoare (predictie)\n",
    "        self.output = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aplică ReLU pe fiecare layer, exceptând layerul de output\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # x = torch.relu(self.fc2(x))\n",
    "        # x = torch.relu(self.fc3(x))\n",
    "        # x = torch.relu(self.fc4(x))\n",
    "        x = self.output(x)  # Nicio funcție de activare (liniară)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creează o instanță a modelului\n",
    "net = DNN().to(device)\n",
    "\n",
    "# Specifică optimizatorul și funcția de pierdere\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # Learning rate de 0.001\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 359225456895.0339\n",
      "Validation Loss: 347081937690.7463\n",
      "Epoch 2/100, Training Loss: 359858191147.4717\n",
      "Validation Loss: 347060828007.1642\n",
      "Epoch 3/100, Training Loss: 360132294725.5547\n",
      "Validation Loss: 347022187199.0448\n",
      "Epoch 4/100, Training Loss: 359306092389.4340\n",
      "Validation Loss: 346967294930.1492\n",
      "Epoch 5/100, Training Loss: 358867613301.8566\n",
      "Validation Loss: 346898483597.3731\n",
      "Epoch 6/100, Training Loss: 363053157457.1472\n",
      "Validation Loss: 346816646250.9850\n",
      "Epoch 7/100, Training Loss: 360319879964.0151\n",
      "Validation Loss: 346722689390.8060\n",
      "Epoch 8/100, Training Loss: 359213797113.2377\n",
      "Validation Loss: 346617973133.3731\n",
      "Epoch 9/100, Training Loss: 360138125327.4566\n",
      "Validation Loss: 346502497600.9552\n",
      "Epoch 10/100, Training Loss: 359790730985.7811\n",
      "Validation Loss: 346378275580.1791\n",
      "Epoch 11/100, Training Loss: 359402005500.1359\n",
      "Validation Loss: 346243923478.9254\n",
      "Epoch 12/100, Training Loss: 359227215732.8906\n",
      "Validation Loss: 346100282505.5522\n",
      "Epoch 13/100, Training Loss: 357986649609.6604\n",
      "Validation Loss: 345948113782.4478\n",
      "Epoch 14/100, Training Loss: 358239628461.8868\n",
      "Validation Loss: 345787120441.3134\n",
      "Epoch 15/100, Training Loss: 357655242806.0981\n",
      "Validation Loss: 345618450462.5671\n",
      "Epoch 16/100, Training Loss: 357969236261.6755\n",
      "Validation Loss: 345440200979.1045\n",
      "Epoch 17/100, Training Loss: 357331123976.6943\n",
      "Validation Loss: 345252997517.3731\n",
      "Epoch 18/100, Training Loss: 357873972019.2000\n",
      "Validation Loss: 345059202063.2836\n",
      "Epoch 19/100, Training Loss: 357048717296.5434\n",
      "Validation Loss: 344856673325.8508\n",
      "Epoch 20/100, Training Loss: 356922338787.0189\n",
      "Validation Loss: 344645492369.1940\n",
      "Epoch 21/100, Training Loss: 356839615785.5396\n",
      "Validation Loss: 344429267096.8358\n",
      "Epoch 22/100, Training Loss: 356938676602.6868\n",
      "Validation Loss: 344203298479.7612\n",
      "Epoch 23/100, Training Loss: 356428525212.4981\n",
      "Validation Loss: 343970426818.8657\n",
      "Epoch 24/100, Training Loss: 355665114741.8566\n",
      "Validation Loss: 343729077660.6567\n",
      "Epoch 25/100, Training Loss: 356212726664.2113\n",
      "Validation Loss: 343482169068.8955\n",
      "Epoch 26/100, Training Loss: 355834170078.1887\n",
      "Validation Loss: 343223712813.8508\n",
      "Epoch 27/100, Training Loss: 355077281397.8566\n",
      "Validation Loss: 342961089520.7164\n",
      "Epoch 28/100, Training Loss: 355140079167.7585\n",
      "Validation Loss: 342690893273.7910\n",
      "Epoch 29/100, Training Loss: 354982130792.3321\n",
      "Validation Loss: 342414540677.7313\n",
      "Epoch 30/100, Training Loss: 356382157565.1019\n",
      "Validation Loss: 342128498412.8955\n",
      "Epoch 31/100, Training Loss: 354997865244.0151\n",
      "Validation Loss: 341837795144.5970\n",
      "Epoch 32/100, Training Loss: 353845297152.0000\n",
      "Validation Loss: 341539805153.4329\n",
      "Epoch 33/100, Training Loss: 353297333901.0415\n",
      "Validation Loss: 341233372022.4478\n",
      "Epoch 34/100, Training Loss: 353319371926.7019\n",
      "Validation Loss: 340921628152.3582\n",
      "Epoch 35/100, Training Loss: 352861117258.3849\n",
      "Validation Loss: 340601014180.2985\n",
      "Epoch 36/100, Training Loss: 352279791855.5773\n",
      "Validation Loss: 340274805805.8508\n",
      "Epoch 37/100, Training Loss: 352062190016.2415\n",
      "Validation Loss: 339941013106.6269\n",
      "Epoch 38/100, Training Loss: 354517491990.2189\n",
      "Validation Loss: 339601630773.4926\n",
      "Epoch 39/100, Training Loss: 351169837898.3849\n",
      "Validation Loss: 339249159213.8508\n",
      "Epoch 40/100, Training Loss: 350862038645.8566\n",
      "Validation Loss: 338896497648.7164\n",
      "Epoch 41/100, Training Loss: 351711817867.1094\n",
      "Validation Loss: 338536569779.5821\n",
      "Epoch 42/100, Training Loss: 350273459458.8981\n",
      "Validation Loss: 338167271485.1343\n",
      "Epoch 43/100, Training Loss: 350137390215.2453\n",
      "Validation Loss: 337792156381.6119\n",
      "Epoch 44/100, Training Loss: 349247517212.9811\n",
      "Validation Loss: 337411242809.3134\n",
      "Epoch 45/100, Training Loss: 349519358469.7962\n",
      "Validation Loss: 337024213114.2687\n",
      "Epoch 46/100, Training Loss: 350253856033.8113\n",
      "Validation Loss: 336627323430.2090\n",
      "Epoch 47/100, Training Loss: 348938320745.2981\n",
      "Validation Loss: 336224095828.0597\n",
      "Epoch 48/100, Training Loss: 347643686494.6717\n",
      "Validation Loss: 335815432283.7015\n",
      "Epoch 49/100, Training Loss: 347311999277.4037\n",
      "Validation Loss: 335403359400.1194\n",
      "Epoch 50/100, Training Loss: 347735137160.2113\n",
      "Validation Loss: 334983400310.4478\n",
      "Epoch 51/100, Training Loss: 347319936799.8793\n",
      "Validation Loss: 334553784809.0746\n",
      "Epoch 52/100, Training Loss: 346713947727.2151\n",
      "Validation Loss: 334118292220.1791\n",
      "Epoch 53/100, Training Loss: 345441190382.6113\n",
      "Validation Loss: 333674962088.1194\n",
      "Epoch 54/100, Training Loss: 346372068054.4604\n",
      "Validation Loss: 333227475677.6119\n",
      "Epoch 55/100, Training Loss: 347702233451.2302\n",
      "Validation Loss: 332768865020.1791\n",
      "Epoch 56/100, Training Loss: 344173781393.8717\n",
      "Validation Loss: 332305875310.8060\n",
      "Epoch 57/100, Training Loss: 343641945872.4227\n",
      "Validation Loss: 331837855835.7015\n",
      "Epoch 58/100, Training Loss: 343054580894.4302\n",
      "Validation Loss: 331362078292.0597\n",
      "Epoch 59/100, Training Loss: 342853574501.4340\n",
      "Validation Loss: 330883395156.0597\n",
      "Epoch 60/100, Training Loss: 345561570864.3019\n",
      "Validation Loss: 330397548054.9254\n",
      "Epoch 61/100, Training Loss: 343099784906.8679\n",
      "Validation Loss: 329903856074.5074\n",
      "Epoch 62/100, Training Loss: 340890698408.0906\n",
      "Validation Loss: 329403076669.1343\n",
      "Epoch 63/100, Training Loss: 342618466450.8377\n",
      "Validation Loss: 328901404702.5671\n",
      "Epoch 64/100, Training Loss: 340891823339.7132\n",
      "Validation Loss: 328387231988.5373\n",
      "Epoch 65/100, Training Loss: 339264927349.8566\n",
      "Validation Loss: 327870508383.5224\n",
      "Epoch 66/100, Training Loss: 338768526432.6038\n",
      "Validation Loss: 327346582421.0150\n",
      "Epoch 67/100, Training Loss: 338842537906.7170\n",
      "Validation Loss: 326818468634.7463\n",
      "Epoch 68/100, Training Loss: 338066572067.7434\n",
      "Validation Loss: 326281141660.6567\n",
      "Epoch 69/100, Training Loss: 337218442437.0717\n",
      "Validation Loss: 325736540435.1045\n",
      "Epoch 70/100, Training Loss: 338898823376.6641\n",
      "Validation Loss: 325183822771.5821\n",
      "Epoch 71/100, Training Loss: 337144764145.5095\n",
      "Validation Loss: 324626066538.9850\n",
      "Epoch 72/100, Training Loss: 337351160998.1585\n",
      "Validation Loss: 324064220175.2836\n",
      "Epoch 73/100, Training Loss: 335679406018.1736\n",
      "Validation Loss: 323500871252.0597\n",
      "Epoch 74/100, Training Loss: 334837274229.8566\n",
      "Validation Loss: 322928737448.1194\n",
      "Epoch 75/100, Training Loss: 334562756600.2717\n",
      "Validation Loss: 322351010097.6716\n",
      "Epoch 76/100, Training Loss: 333210204724.1660\n",
      "Validation Loss: 321766485221.2537\n",
      "Epoch 77/100, Training Loss: 332589870377.5396\n",
      "Validation Loss: 321182417629.6119\n",
      "Epoch 78/100, Training Loss: 331938009938.1132\n",
      "Validation Loss: 320581972885.0150\n",
      "Epoch 79/100, Training Loss: 331204643024.6641\n",
      "Validation Loss: 319979811243.9403\n",
      "Epoch 80/100, Training Loss: 330477175896.8755\n",
      "Validation Loss: 319372741005.3731\n",
      "Epoch 81/100, Training Loss: 330293815071.8793\n",
      "Validation Loss: 318756927824.2388\n",
      "Epoch 82/100, Training Loss: 330226296298.7472\n",
      "Validation Loss: 318130523976.5970\n",
      "Epoch 83/100, Training Loss: 328840101084.2566\n",
      "Validation Loss: 317505361155.8209\n",
      "Epoch 84/100, Training Loss: 329278983805.5849\n",
      "Validation Loss: 316868077109.4926\n",
      "Epoch 85/100, Training Loss: 328486278464.7245\n",
      "Validation Loss: 316225128234.0298\n",
      "Epoch 86/100, Training Loss: 327136903044.3472\n",
      "Validation Loss: 315580014958.8060\n",
      "Epoch 87/100, Training Loss: 326126953220.8302\n",
      "Validation Loss: 314926034638.3284\n",
      "Epoch 88/100, Training Loss: 326882516651.9547\n",
      "Validation Loss: 314273849221.7313\n",
      "Epoch 89/100, Training Loss: 324840169317.4340\n",
      "Validation Loss: 313606890572.4179\n",
      "Epoch 90/100, Training Loss: 324641632456.9359\n",
      "Validation Loss: 312943560979.1045\n",
      "Epoch 91/100, Training Loss: 324075433875.8038\n",
      "Validation Loss: 312272512320.9552\n",
      "Epoch 92/100, Training Loss: 322631766247.8491\n",
      "Validation Loss: 311596590645.4926\n",
      "Epoch 93/100, Training Loss: 322236373896.2113\n",
      "Validation Loss: 310910583028.5373\n",
      "Epoch 94/100, Training Loss: 321447232848.1812\n",
      "Validation Loss: 310217241370.7463\n",
      "Epoch 95/100, Training Loss: 321800726118.4000\n",
      "Validation Loss: 309525146486.4478\n",
      "Epoch 96/100, Training Loss: 320943537743.2151\n",
      "Validation Loss: 308826154427.2239\n",
      "Epoch 97/100, Training Loss: 319389723945.5396\n",
      "Validation Loss: 308120791926.4478\n",
      "Epoch 98/100, Training Loss: 318210526165.4943\n",
      "Validation Loss: 307408575900.6567\n",
      "Epoch 99/100, Training Loss: 317633291816.5736\n",
      "Validation Loss: 306690230913.9105\n",
      "Epoch 100/100, Training Loss: 317251465293.2830\n",
      "Validation Loss: 305970535347.5821\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, loss_function, epochs):\n",
    "    model.to(device)  # Asigură-te că modelul este pe dispozitivul corect\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(\n",
    "                device)  # Transferă datele pe GPU\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}, Training Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        # Evaluare pe setul de validare\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(\n",
    "                    device)  # Transferă datele pe GPU\n",
    "                outputs = model(inputs)\n",
    "                val_loss += loss_function(outputs, targets.view(-1, 1)).item()\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Exemplu de antrenare a modelului\n",
    "train_model(net, train_loader, val_loader, optimizer, loss_function, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy (±10%): 0.00%\n",
      "Validation Accuracy (±10%): 0.00%\n"
     ]
    }
   ],
   "source": [
    "def regression_accuracy(model, data_loader, tolerance=0.1):\n",
    "    total_samples = 0\n",
    "    accurate_predictions = 0\n",
    "    model.to(device)  # Transferă modelul pe dispozitivul corect\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Transferă datele pe GPU\n",
    "            outputs = model(inputs)\n",
    "            # Calculăm acuratețea în funcție de toleranță\n",
    "            accurate_predictions += ((outputs.squeeze() - targets).abs() / targets <= tolerance).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    accuracy = (accurate_predictions / total_samples) * 100\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "# Calculul acurateței\n",
    "train_accuracy = regression_accuracy(net, train_loader, tolerance=0.1)\n",
    "val_accuracy = regression_accuracy(net, val_loader, tolerance=0.1)\n",
    "print(f\"Train Accuracy (±10%): {train_accuracy:.2f}%\")\n",
    "print(f\"Validation Accuracy (±10%): {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispozitiv utilizat: cuda\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Convertim datele în tensori PyTorch și îi mutăm pe dispozitivul CUDA (GPU)\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mX_train\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Ne asigurăm că are dimensiunea corectă\u001b[39;00m\n\u001b[0;32m     41\u001b[0m y_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m     42\u001b[0m     y_train\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Verificăm dacă există disponibilitate CUDA (GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispozitiv utilizat:\", device)\n",
    "\n",
    "# Definim modelul rețelei neurale\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(13, 30)\n",
    "        self.fc2 = nn.Linear(30, 30)\n",
    "        self.fc3 = nn.Linear(30, 30)\n",
    "        self.fc4 = nn.Linear(30, 30)\n",
    "        self.fc5 = nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        # Funcția de activare pentru stratul de ieșire este 'linear'\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)  # Mutăm modelul pe dispozitivul CUDA (GPU)\n",
    "\n",
    "# Definim funcția de pierdere și optimizatorul\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Convertim datele în tensori PyTorch și îi mutăm pe dispozitivul CUDA (GPU)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "# Ne asigurăm că are dimensiunea corectă\n",
    "y_train_tensor = torch.tensor(\n",
    "    y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Creăm un TensorDataset și un DataLoader pentru setul de antrenare\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Antrenăm modelul\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Convertim datele de testare în tensori PyTorch și îi mutăm pe dispozitivul CUDA (GPU)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "# Ne asigurăm că are dimensiunea corectă\n",
    "y_test_tensor = torch.tensor(\n",
    "    y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Evaluăm performanța modelului pe setul de testare\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    test_loss = criterion(y_pred, y_test_tensor)\n",
    "print(\"Loss pe setul de testare:\", test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculăm coeficientul de determinare (R^2)\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2.item()\n",
    "\n",
    "\n",
    "# Calculăm coeficientul de determinare pentru setul de testare\n",
    "r2 = r_squared(y_test_tensor, y_pred)\n",
    "print(\"Coeficientul de determinare (R^2) pe setul de testare:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Încarcă setul de date CSV într-un DataFrame\n",
    "df = pd.read_csv(\"../data/final_dataset_1.csv\")\n",
    "\n",
    "\n",
    "# Separă caracteristicile de variabila țintă (dacă există)\n",
    "# Excludem coloana 'price' care este variabila țintă\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']  # Definim variabila țintă\n",
    "\n",
    "# Standardizăm datele\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Aplicăm PCA\n",
    "pca = PCA(n_components=13)  # Specificăm să reducem la 13 componente\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Verificăm câte componente ne-au rămas\n",
    "print(\"Numărul de componente după PCA:\", X_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test = train_test_split(X_pca, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Dimensiunea setului de date de antrenament:\", X_train.shape)\n",
    "print(\"Dimensiunea setului de date de testare:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(13,30),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(30,30),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(30,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "trans = [transforms.ToTensor()]\n",
    "trans = transforms.Compose(trans)\n",
    "\n",
    "train, val = torch.utils.data.random_split(X_train, [len(X_train) - len(X_train) // 10, len(X_train) // 10], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_iter = DataLoader(train, batch_size, shuffle=True)\n",
    "val_iter = DataLoader(val, batch_size, shuffle=False)\n",
    "test_iter = DataLoader(X_test, batch_size, shuffle=False)\n",
    "\n",
    "# show the length of train_iter\n",
    "print(len(train))\n",
    "len(X_train),len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter):\n",
    "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    total_loss = 0\n",
    "    total_hits = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            total_loss += float(l)\n",
    "            total_hits += sum(net(X).argmax(axis=1).type(y.dtype) == y)\n",
    "            total_samples += y.numel()\n",
    "    return float(total_loss) / len(data_iter), float(total_hits) / total_samples * 100\n",
    "\n",
    "def train_epoch(net, train_iter, loss, optimizer):\n",
    "    # Set the model to training mode\n",
    "    net.train()\n",
    "    # Sum of training loss, sum of training correct predictions, no. of examples\n",
    "    total_loss = 0\n",
    "    total_hits = 0\n",
    "    total_samples = 0\n",
    "    for X, y in train_iter:\n",
    "        # Compute gradients and update parameters\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        # Using PyTorch built-in optimizer & loss criterion\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(l)\n",
    "        total_hits += sum(y_hat.argmax(axis=1).type(y.dtype) == y)\n",
    "        total_samples += y.numel()\n",
    "    # Return training loss and training accuracy\n",
    "    return float(total_loss) / len(train_iter), float(total_hits) / total_samples * 100\n",
    "\n",
    "\n",
    "def train(net, train_iter, val_iter, loss, num_epochs, optimizer):\n",
    "    \"\"\"Train a model.\"\"\"\n",
    "    train_loss_all = []\n",
    "    train_acc_all = []\n",
    "    val_loss_all = []\n",
    "    val_acc_all = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(net, train_iter, loss, optimizer)\n",
    "        train_loss_all.append(train_loss)\n",
    "        train_acc_all.append(train_acc)\n",
    "        val_loss, val_acc = evaluate_accuracy(net, val_iter)\n",
    "        val_loss_all.append(val_loss)\n",
    "        val_acc_all.append(val_acc)\n",
    "        print(f'Epoch {epoch + 1}, Train loss {train_loss:.2f}, Train accuracy {train_acc:.2f}, Validation loss {val_loss:.2f}, Validation accuracy {val_acc:.2f}')\n",
    "\n",
    "    return train_loss_all, train_acc_all, val_loss_all, val_acc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in X_train:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "num_epochs = 50\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "train_loss_all, train_acc_all, val_loss_all, val_acc_all = train(\n",
    "    net, train_iter, val_iter, loss, num_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
