{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN time\n",
    "\n",
    "### For this dataset we are going with a DNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "devices = [d for d in range(torch.cuda.device_count())]\n",
    "device_names = [torch.cuda.get_device_name(d) for d in devices]\n",
    "print(device_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../data/final_dataset_1.csv')\n",
    "\n",
    "X = data.drop('price', axis=1).values\n",
    "y = data['price'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model and its losing and optimizing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not use this model, use the next cell\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(X_train.shape[1], 128)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout layer\n",
    "        self.layer2 = nn.Linear(128, 256)\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)  # Applying dropout\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = DNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definirea modelului DNN\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        # Inițializarea layerelor\n",
    "        # Primul layer primeste 13 caracteristici (după PCA)\n",
    "        self.fc1 = nn.Linear(16, 30)\n",
    "        self.fc2 = nn.Linear(30, 30)  # Al doilea layer\n",
    "        self.fc3 = nn.Linear(30, 30)  # Al treilea layer\n",
    "        self.fc4 = nn.Linear(30, 20)  # Al patrulea layer\n",
    "        # Layerul de output cu o singură valoare (predictie)\n",
    "        self.output = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aplică ReLU pe fiecare layer, exceptând layerul de output\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.output(x)  # Nicio funcție de activare (liniară)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = DNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 248009277440.0000\n",
      "Epoch [2/300], Loss: 44898463744.0000\n",
      "Epoch [3/300], Loss: 54407749632.0000\n",
      "Epoch [4/300], Loss: 23964289024.0000\n",
      "Epoch [5/300], Loss: 147291291648.0000\n",
      "Epoch [6/300], Loss: 17637040128.0000\n",
      "Epoch [7/300], Loss: 396567347200.0000\n",
      "Epoch [8/300], Loss: 79004721152.0000\n",
      "Epoch [9/300], Loss: 14989769728.0000\n",
      "Epoch [10/300], Loss: 64486006784.0000\n",
      "Epoch [11/300], Loss: 33806104576.0000\n",
      "Epoch [12/300], Loss: 11338874880.0000\n",
      "Epoch [13/300], Loss: 23598143488.0000\n",
      "Epoch [14/300], Loss: 22119321600.0000\n",
      "Epoch [15/300], Loss: 53801377792.0000\n",
      "Epoch [16/300], Loss: 35688972288.0000\n",
      "Epoch [17/300], Loss: 32804685824.0000\n",
      "Epoch [18/300], Loss: 29121413120.0000\n",
      "Epoch [19/300], Loss: 18499639296.0000\n",
      "Epoch [20/300], Loss: 29887412224.0000\n",
      "Epoch [21/300], Loss: 24052111360.0000\n",
      "Epoch [22/300], Loss: 65485209600.0000\n",
      "Epoch [23/300], Loss: 24495118336.0000\n",
      "Epoch [24/300], Loss: 10590191616.0000\n",
      "Epoch [25/300], Loss: 49799364608.0000\n",
      "Epoch [26/300], Loss: 8071421952.0000\n",
      "Epoch [27/300], Loss: 84060004352.0000\n",
      "Epoch [28/300], Loss: 24523044864.0000\n",
      "Epoch [29/300], Loss: 24960448512.0000\n",
      "Epoch [30/300], Loss: 17389135872.0000\n",
      "Epoch [31/300], Loss: 22043262976.0000\n",
      "Epoch [32/300], Loss: 9411918848.0000\n",
      "Epoch [33/300], Loss: 21022844928.0000\n",
      "Epoch [34/300], Loss: 37561864192.0000\n",
      "Epoch [35/300], Loss: 14255543296.0000\n",
      "Epoch [36/300], Loss: 21037359104.0000\n",
      "Epoch [37/300], Loss: 16561071104.0000\n",
      "Epoch [38/300], Loss: 14159406080.0000\n",
      "Epoch [39/300], Loss: 26971140096.0000\n",
      "Epoch [40/300], Loss: 18254405632.0000\n",
      "Epoch [41/300], Loss: 12660960256.0000\n",
      "Epoch [42/300], Loss: 16954231808.0000\n",
      "Epoch [43/300], Loss: 23766177792.0000\n",
      "Epoch [44/300], Loss: 18457153536.0000\n",
      "Epoch [45/300], Loss: 17009004544.0000\n",
      "Epoch [46/300], Loss: 17607247872.0000\n",
      "Epoch [47/300], Loss: 22625927168.0000\n",
      "Epoch [48/300], Loss: 98892619776.0000\n",
      "Epoch [49/300], Loss: 68967047168.0000\n",
      "Epoch [50/300], Loss: 23291052032.0000\n",
      "Epoch [51/300], Loss: 13721217024.0000\n",
      "Epoch [52/300], Loss: 72620621824.0000\n",
      "Epoch [53/300], Loss: 24768698368.0000\n",
      "Epoch [54/300], Loss: 15925920768.0000\n",
      "Epoch [55/300], Loss: 91276951552.0000\n",
      "Epoch [56/300], Loss: 28082149376.0000\n",
      "Epoch [57/300], Loss: 59687530496.0000\n",
      "Epoch [58/300], Loss: 20021274624.0000\n",
      "Epoch [59/300], Loss: 33197086720.0000\n",
      "Epoch [60/300], Loss: 30258860032.0000\n",
      "Epoch [61/300], Loss: 17307179008.0000\n",
      "Epoch [62/300], Loss: 29579798528.0000\n",
      "Epoch [63/300], Loss: 33250734080.0000\n",
      "Epoch [64/300], Loss: 16217262080.0000\n",
      "Epoch [65/300], Loss: 24424566784.0000\n",
      "Epoch [66/300], Loss: 16796617728.0000\n",
      "Epoch [67/300], Loss: 25444438016.0000\n",
      "Epoch [68/300], Loss: 13358526464.0000\n",
      "Epoch [69/300], Loss: 13767149568.0000\n",
      "Epoch [70/300], Loss: 16492670976.0000\n",
      "Epoch [71/300], Loss: 32231532544.0000\n",
      "Epoch [72/300], Loss: 76254904320.0000\n",
      "Epoch [73/300], Loss: 32397002752.0000\n",
      "Epoch [74/300], Loss: 50240434176.0000\n",
      "Epoch [75/300], Loss: 21430937600.0000\n",
      "Epoch [76/300], Loss: 13092154368.0000\n",
      "Epoch [77/300], Loss: 36709023744.0000\n",
      "Epoch [78/300], Loss: 16999205888.0000\n",
      "Epoch [79/300], Loss: 55481765888.0000\n",
      "Epoch [80/300], Loss: 28393336832.0000\n",
      "Epoch [81/300], Loss: 21859774464.0000\n",
      "Epoch [82/300], Loss: 30847723520.0000\n",
      "Epoch [83/300], Loss: 14648577024.0000\n",
      "Epoch [84/300], Loss: 16925800448.0000\n",
      "Epoch [85/300], Loss: 21761441792.0000\n",
      "Epoch [86/300], Loss: 97015914496.0000\n",
      "Epoch [87/300], Loss: 29719271424.0000\n",
      "Epoch [88/300], Loss: 31157839872.0000\n",
      "Epoch [89/300], Loss: 31517501440.0000\n",
      "Epoch [90/300], Loss: 48072642560.0000\n",
      "Epoch [91/300], Loss: 14651772928.0000\n",
      "Epoch [92/300], Loss: 20985747456.0000\n",
      "Epoch [93/300], Loss: 21139154944.0000\n",
      "Epoch [94/300], Loss: 59473375232.0000\n",
      "Epoch [95/300], Loss: 17960652800.0000\n",
      "Epoch [96/300], Loss: 40455172096.0000\n",
      "Epoch [97/300], Loss: 10160626688.0000\n",
      "Epoch [98/300], Loss: 19224621056.0000\n",
      "Epoch [99/300], Loss: 44110344192.0000\n",
      "Epoch [100/300], Loss: 34158546944.0000\n",
      "Epoch [101/300], Loss: 15202674688.0000\n",
      "Epoch [102/300], Loss: 21713879040.0000\n",
      "Epoch [103/300], Loss: 18320930816.0000\n",
      "Epoch [104/300], Loss: 42203758592.0000\n",
      "Epoch [105/300], Loss: 30734360576.0000\n",
      "Epoch [106/300], Loss: 33351002112.0000\n",
      "Epoch [107/300], Loss: 23507083264.0000\n",
      "Epoch [108/300], Loss: 20941623296.0000\n",
      "Epoch [109/300], Loss: 8180139008.0000\n",
      "Epoch [110/300], Loss: 28797626368.0000\n",
      "Epoch [111/300], Loss: 26847799296.0000\n",
      "Epoch [112/300], Loss: 24669542400.0000\n",
      "Epoch [113/300], Loss: 15654805504.0000\n",
      "Epoch [114/300], Loss: 15933122560.0000\n",
      "Epoch [115/300], Loss: 212480278528.0000\n",
      "Epoch [116/300], Loss: 17133345792.0000\n",
      "Epoch [117/300], Loss: 12934767616.0000\n",
      "Epoch [118/300], Loss: 50751139840.0000\n",
      "Epoch [119/300], Loss: 43768852480.0000\n",
      "Epoch [120/300], Loss: 20689321984.0000\n",
      "Epoch [121/300], Loss: 32177821696.0000\n",
      "Epoch [122/300], Loss: 28325859328.0000\n",
      "Epoch [123/300], Loss: 39772352512.0000\n",
      "Epoch [124/300], Loss: 18767433728.0000\n",
      "Epoch [125/300], Loss: 26335207424.0000\n",
      "Epoch [126/300], Loss: 29050712064.0000\n",
      "Epoch [127/300], Loss: 26516987904.0000\n",
      "Epoch [128/300], Loss: 35725479936.0000\n",
      "Epoch [129/300], Loss: 76249571328.0000\n",
      "Epoch [130/300], Loss: 81607868416.0000\n",
      "Epoch [131/300], Loss: 41431715840.0000\n",
      "Epoch [132/300], Loss: 27483234304.0000\n",
      "Epoch [133/300], Loss: 34815225856.0000\n",
      "Epoch [134/300], Loss: 14725795840.0000\n",
      "Epoch [135/300], Loss: 12602656768.0000\n",
      "Epoch [136/300], Loss: 14140800000.0000\n",
      "Epoch [137/300], Loss: 69231288320.0000\n",
      "Epoch [138/300], Loss: 12593372160.0000\n",
      "Epoch [139/300], Loss: 16879023104.0000\n",
      "Epoch [140/300], Loss: 23435282432.0000\n",
      "Epoch [141/300], Loss: 177841225728.0000\n",
      "Epoch [142/300], Loss: 15989633024.0000\n",
      "Epoch [143/300], Loss: 28708667392.0000\n",
      "Epoch [144/300], Loss: 63776665600.0000\n",
      "Epoch [145/300], Loss: 30131515392.0000\n",
      "Epoch [146/300], Loss: 12964013056.0000\n",
      "Epoch [147/300], Loss: 15644710912.0000\n",
      "Epoch [148/300], Loss: 48671944704.0000\n",
      "Epoch [149/300], Loss: 62129778688.0000\n",
      "Epoch [150/300], Loss: 19369383936.0000\n",
      "Epoch [151/300], Loss: 24951992320.0000\n",
      "Epoch [152/300], Loss: 16557889536.0000\n",
      "Epoch [153/300], Loss: 46436052992.0000\n",
      "Epoch [154/300], Loss: 24226818048.0000\n",
      "Epoch [155/300], Loss: 62418362368.0000\n",
      "Epoch [156/300], Loss: 22430967808.0000\n",
      "Epoch [157/300], Loss: 14227719168.0000\n",
      "Epoch [158/300], Loss: 34561875968.0000\n",
      "Epoch [159/300], Loss: 22587072512.0000\n",
      "Epoch [160/300], Loss: 76512731136.0000\n",
      "Epoch [161/300], Loss: 35127848960.0000\n",
      "Epoch [162/300], Loss: 25639921664.0000\n",
      "Epoch [163/300], Loss: 34147041280.0000\n",
      "Epoch [164/300], Loss: 51915550720.0000\n",
      "Epoch [165/300], Loss: 43690369024.0000\n",
      "Epoch [166/300], Loss: 35361103872.0000\n",
      "Epoch [167/300], Loss: 17594486784.0000\n",
      "Epoch [168/300], Loss: 68244287488.0000\n",
      "Epoch [169/300], Loss: 54348197888.0000\n",
      "Epoch [170/300], Loss: 28004237312.0000\n",
      "Epoch [171/300], Loss: 11300624384.0000\n",
      "Epoch [172/300], Loss: 9356102656.0000\n",
      "Epoch [173/300], Loss: 28708515840.0000\n",
      "Epoch [174/300], Loss: 16521251840.0000\n",
      "Epoch [175/300], Loss: 14630020096.0000\n",
      "Epoch [176/300], Loss: 111167610880.0000\n",
      "Epoch [177/300], Loss: 27545004032.0000\n",
      "Epoch [178/300], Loss: 15970244608.0000\n",
      "Epoch [179/300], Loss: 59068575744.0000\n",
      "Epoch [180/300], Loss: 30288627712.0000\n",
      "Epoch [181/300], Loss: 20346750976.0000\n",
      "Epoch [182/300], Loss: 57192284160.0000\n",
      "Epoch [183/300], Loss: 15345410048.0000\n",
      "Epoch [184/300], Loss: 26899357696.0000\n",
      "Epoch [185/300], Loss: 50233212928.0000\n",
      "Epoch [186/300], Loss: 10023699456.0000\n",
      "Epoch [187/300], Loss: 36955287552.0000\n",
      "Epoch [188/300], Loss: 8187679232.0000\n",
      "Epoch [189/300], Loss: 29339084800.0000\n",
      "Epoch [190/300], Loss: 79211192320.0000\n",
      "Epoch [191/300], Loss: 13453725696.0000\n",
      "Epoch [192/300], Loss: 144074653696.0000\n",
      "Epoch [193/300], Loss: 34280120320.0000\n",
      "Epoch [194/300], Loss: 11579010048.0000\n",
      "Epoch [195/300], Loss: 31506651136.0000\n",
      "Epoch [196/300], Loss: 117846147072.0000\n",
      "Epoch [197/300], Loss: 116114751488.0000\n",
      "Epoch [198/300], Loss: 20128876544.0000\n",
      "Epoch [199/300], Loss: 15232943104.0000\n",
      "Epoch [200/300], Loss: 35098763264.0000\n",
      "Epoch [201/300], Loss: 19046031360.0000\n",
      "Epoch [202/300], Loss: 20293896192.0000\n",
      "Epoch [203/300], Loss: 21690640384.0000\n",
      "Epoch [204/300], Loss: 21917829120.0000\n",
      "Epoch [205/300], Loss: 9698093056.0000\n",
      "Epoch [206/300], Loss: 42523099136.0000\n",
      "Epoch [207/300], Loss: 49535922176.0000\n",
      "Epoch [208/300], Loss: 24332343296.0000\n",
      "Epoch [209/300], Loss: 18619252736.0000\n",
      "Epoch [210/300], Loss: 16305165312.0000\n",
      "Epoch [211/300], Loss: 26429259776.0000\n",
      "Epoch [212/300], Loss: 46557069312.0000\n",
      "Epoch [213/300], Loss: 18071324672.0000\n",
      "Epoch [214/300], Loss: 19812327424.0000\n",
      "Epoch [215/300], Loss: 64509173760.0000\n",
      "Epoch [216/300], Loss: 17046984704.0000\n",
      "Epoch [217/300], Loss: 104942034944.0000\n",
      "Epoch [218/300], Loss: 30471079936.0000\n",
      "Epoch [219/300], Loss: 71043309568.0000\n",
      "Epoch [220/300], Loss: 21128138752.0000\n",
      "Epoch [221/300], Loss: 13741818880.0000\n",
      "Epoch [222/300], Loss: 31240374272.0000\n",
      "Epoch [223/300], Loss: 10383368192.0000\n",
      "Epoch [224/300], Loss: 30798731264.0000\n",
      "Epoch [225/300], Loss: 17927770112.0000\n",
      "Epoch [226/300], Loss: 28744562688.0000\n",
      "Epoch [227/300], Loss: 23456876544.0000\n",
      "Epoch [228/300], Loss: 32189001728.0000\n",
      "Epoch [229/300], Loss: 7587724288.0000\n",
      "Epoch [230/300], Loss: 74224558080.0000\n",
      "Epoch [231/300], Loss: 23270021120.0000\n",
      "Epoch [232/300], Loss: 63270318080.0000\n",
      "Epoch [233/300], Loss: 23857016832.0000\n",
      "Epoch [234/300], Loss: 20995129344.0000\n",
      "Epoch [235/300], Loss: 10147222528.0000\n",
      "Epoch [236/300], Loss: 92421554176.0000\n",
      "Epoch [237/300], Loss: 13915737088.0000\n",
      "Epoch [238/300], Loss: 29261875200.0000\n",
      "Epoch [239/300], Loss: 12744440832.0000\n",
      "Epoch [240/300], Loss: 36369702912.0000\n",
      "Epoch [241/300], Loss: 19770376192.0000\n",
      "Epoch [242/300], Loss: 20311076864.0000\n",
      "Epoch [243/300], Loss: 50483470336.0000\n",
      "Epoch [244/300], Loss: 21820573696.0000\n",
      "Epoch [245/300], Loss: 13474728960.0000\n",
      "Epoch [246/300], Loss: 53233848320.0000\n",
      "Epoch [247/300], Loss: 18299678720.0000\n",
      "Epoch [248/300], Loss: 21040130048.0000\n",
      "Epoch [249/300], Loss: 32498458624.0000\n",
      "Epoch [250/300], Loss: 20217741312.0000\n",
      "Epoch [251/300], Loss: 24131223552.0000\n",
      "Epoch [252/300], Loss: 11865125888.0000\n",
      "Epoch [253/300], Loss: 17905012736.0000\n",
      "Epoch [254/300], Loss: 11628881920.0000\n",
      "Epoch [255/300], Loss: 23781523456.0000\n",
      "Epoch [256/300], Loss: 24567349248.0000\n",
      "Epoch [257/300], Loss: 31668490240.0000\n",
      "Epoch [258/300], Loss: 22352306176.0000\n",
      "Epoch [259/300], Loss: 25060175872.0000\n",
      "Epoch [260/300], Loss: 22169804800.0000\n",
      "Epoch [261/300], Loss: 14968376320.0000\n",
      "Epoch [262/300], Loss: 34356494336.0000\n",
      "Epoch [263/300], Loss: 20349470720.0000\n",
      "Epoch [264/300], Loss: 19788204032.0000\n",
      "Epoch [265/300], Loss: 40805376000.0000\n",
      "Epoch [266/300], Loss: 39461789696.0000\n",
      "Epoch [267/300], Loss: 32438665216.0000\n",
      "Epoch [268/300], Loss: 13663991808.0000\n",
      "Epoch [269/300], Loss: 23034411008.0000\n",
      "Epoch [270/300], Loss: 31868903424.0000\n",
      "Epoch [271/300], Loss: 18862458880.0000\n",
      "Epoch [272/300], Loss: 16215388160.0000\n",
      "Epoch [273/300], Loss: 68415504384.0000\n",
      "Epoch [274/300], Loss: 9622612992.0000\n",
      "Epoch [275/300], Loss: 15392054272.0000\n",
      "Epoch [276/300], Loss: 31317168128.0000\n",
      "Epoch [277/300], Loss: 24822712320.0000\n",
      "Epoch [278/300], Loss: 45959364608.0000\n",
      "Epoch [279/300], Loss: 19416578048.0000\n",
      "Epoch [280/300], Loss: 20552429568.0000\n",
      "Epoch [281/300], Loss: 22619887616.0000\n",
      "Epoch [282/300], Loss: 18673985536.0000\n",
      "Epoch [283/300], Loss: 14214010880.0000\n",
      "Epoch [284/300], Loss: 50567712768.0000\n",
      "Epoch [285/300], Loss: 22072301568.0000\n",
      "Epoch [286/300], Loss: 13437372416.0000\n",
      "Epoch [287/300], Loss: 8508985344.0000\n",
      "Epoch [288/300], Loss: 22455650304.0000\n",
      "Epoch [289/300], Loss: 11906952192.0000\n",
      "Epoch [290/300], Loss: 20942651392.0000\n",
      "Epoch [291/300], Loss: 13030047744.0000\n",
      "Epoch [292/300], Loss: 11874803712.0000\n",
      "Epoch [293/300], Loss: 11243727872.0000\n",
      "Epoch [294/300], Loss: 67764400128.0000\n",
      "Epoch [295/300], Loss: 12584393728.0000\n",
      "Epoch [296/300], Loss: 22369564672.0000\n",
      "Epoch [297/300], Loss: 17685927936.0000\n",
      "Epoch [298/300], Loss: 29375797248.0000\n",
      "Epoch [299/300], Loss: 40106926080.0000\n",
      "Epoch [300/300], Loss: 21083488256.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "model.train()  \n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs, targets\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 29288804352.0000\n",
      "Test RMSE Loss: 171139.7188\n",
      "Test MAE Loss: 110256.2031\n",
      "R-squared Score: 0.7416\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculation for evaluation\n",
    "with torch.no_grad():\n",
    "    inputs, targets = X_test, y_test\n",
    "    predictions = model(inputs)\n",
    "\n",
    "    # Calculating different metrics\n",
    "    mse_loss = criterion(predictions, targets)  # MSE\n",
    "    rmse_loss = torch.sqrt(mse_loss)            # RMSE\n",
    "    mae_loss = torch.mean(torch.abs(predictions - targets))  # MAE\n",
    "    r2 = r2_score(targets.cpu().numpy(), predictions.cpu().numpy())  # R2 score\n",
    "\n",
    "    # Printing the metrics\n",
    "    print(f'Test MSE Loss: {mse_loss.item():.4f}')\n",
    "    print(f'Test RMSE Loss: {rmse_loss.item():.4f}')\n",
    "    print(f'Test MAE Loss: {mae_loss.item():.4f}')\n",
    "    print(f'R-squared Score: {r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy (±10%): 789.29%\n",
      "Validation Accuracy (±10%): 786.94%\n"
     ]
    }
   ],
   "source": [
    "def regression_accuracy(model, data_loader, tolerance=0.1):\n",
    "    total_samples = 0\n",
    "    accurate_predictions = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs, targets\n",
    "            outputs = model(inputs)\n",
    "            # Calculăm acuratețea în funcție de toleranță\n",
    "            accurate_predictions += ((outputs.squeeze() -\n",
    "                                     targets).abs() / targets <= tolerance).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    accuracy = (accurate_predictions / total_samples) * 100\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Calculul acurateței\n",
    "train_accuracy = regression_accuracy(model, train_loader, tolerance=0.1)\n",
    "val_accuracy = regression_accuracy(model, train_loader, tolerance=0.1)\n",
    "print(f\"Train Accuracy (±10%): {train_accuracy:.2f}%\")\n",
    "print(f\"Validation Accuracy (±10%): {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "df = pd.read_csv('../data/final_dataset_1.csv')\n",
    "\n",
    "# Assuming df is your DataFrame and 'price' is the target variable\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "clf = GradientBoostingRegressor(n_estimators = 700, max_depth = 7, min_samples_split = 3, learning_rate = 0.1)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "Y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 30194992032.6178\n",
      "Mean Absolute Error: 103340.4322\n",
      "R-squared: 0.7019\n"
     ]
    }
   ],
   "source": [
    "# Add new metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mse = mean_squared_error(y_test, Y_pred)\n",
    "mae = mean_absolute_error(y_test, Y_pred)\n",
    "r2 = r2_score(y_test, Y_pred)\n",
    "#accuracy = accuracy_score(y_test, Y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'Mean Absolute Error: {mae:.4f}')\n",
    "print(f'R-squared: {r2:.4f}')\n",
    "#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Create an XGBoost regressor\n",
    "model = XGBRegressor(objective='reg:squarederror', random_state=2)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "Y_pred = model.predict(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IC_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
